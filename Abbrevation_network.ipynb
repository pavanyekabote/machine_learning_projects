{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "from keras.models import Model, Sequential\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbbrNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model = self._generate_model()\n",
    "        self.vectorizer = self._generate_vectorizer()\n",
    "        self.set_training_data()\n",
    "        \n",
    "    def _generate_model(self):\n",
    "        from keras.layers import Dense, Dropout\n",
    "        from keras.models import Sequential\n",
    "        \n",
    "        model.add(Dense(25, activation='relu', input_shape=(52,)))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(10, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def set_training_data(self):\n",
    "        self._training_data = ['Lorem','IPSUm','DOLOr','SiT','amet','conSectetur','ADIPSiCing','elit','Nullam','sed','HENDrieT','dolor','Aliquam','iaculis','dui','ut','varius','dignissim','Suspendisse','rutrum','sem','quam','Curabitur','cursus','mi','at','nunc','interdum','rhoncus','Donec','hendrerit','sapien','ex','sit','amet','rhoncus','massa','posuere','eu','Vivamus','hendrerit','libero','vitae','risus','posuere','a','rutrum','diam','euismod','Aliquam','a','turpis','ut','neque','laoreet','lobortis','non','in','mauris','Sed','a','nisl','eget','ipsum','feugiat','tincidunt','at','rhoncus','velit','Nunc','iaculis','elit','sed','hendrerit','imperdiet','Suspendisse','auctor','augue','id','efficitur','blandit','Ut','quis','lectus','non','libero','vehicula','efficitur','Aliquam','ut','elementum','mi','vulputate','eleifend','ex','Suspendisse','ornare','finibus','turpis','sit','amet','cursus','Donec','viverra','eget','nisi','eu','sollicitudin','Aenean','posuere','metus','sed','risus','posuere','faucibus','Proin','faucibus','sem','dui','non','mollis','lacus','luctus','quis','Integer','vitae','hendrerit','massa','Pellentesque','habitant','morbi','tristique','senectus','et','netus','et','malesuada','fames','ac','turpis','egestas','Praesent','viverra','sollicitudin','nisi','et','consequat','odio','congue','in','Curabitur','urna','ipsum','elementum','at','congue','ut','faucibus','quis','sem','Morbi','porttitor','ullamcorper','sapien','tincidunt','egestas','nunc','Sed','nibh','ante','bibendum','non','lacus','ut','aliquet','mollis','sapien','Nullam','vel','posuere','ex','Nunc','eget','elementum','purus','Sed','eu','nibh','euismod','porta','ex','vitae','tristique','augue','Phasellus','fringilla','fermentum','justo','non','imperdiet','Vivamus','molestie','porttitor','sapien','sed','efficitur','lorem','ullamcorper','at','Integer','dignissim','magna','a','mauris','fringilla','dapibus','Cras','quis','porta','lacus','id','aliquam','erat','Etiam','dignissim','libero','tempus','placerat','vulputate','turpis','arcu','eleifend','nibh','sagittis','elementum','massa','tellus','vitae','felis','Pellentesque','euismod','risus','vel','sem','cursus','semper','Aliquam','sit','amet','dui','a','magna','euismod','ultricies','rutrum','sed','ex','Donec','a','ligula','lorem','Nam','velit','quam','dignissim','sit','amet','magna','sit','amet','efficitur','pretium','urna','Suspendisse','pretium','aliquet','rhoncus','Aliquam','eros','nunc','pulvinar','at','ante','vel','viverra','auctor','erat']\n",
    "        \n",
    "    \n",
    "    def _generate_vectorizer(self):\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        \n",
    "        #create vectorizer corpus\n",
    "        vectorizer_corpus = [' '.join([chr(i) for i in range(65, 123) if i < 91 or i > 96])]\n",
    "        \n",
    "        #vectorizer \n",
    "        vectorizer = CountVectorizer(binary=True, \n",
    "                                     lowercase=False,\n",
    "                                     analyzer = \"word\",\n",
    "                                     tokenizer = None,\n",
    "                                     preprocessor = None,\n",
    "                                     stop_words = None,\n",
    "                                     max_features = 5000,\n",
    "                                     token_pattern = r\"(?u)\\b\\w+\\b\")\n",
    "        vectorizer.fit(vectorizer_corpus)\n",
    "        return vectorizer\n",
    "    \n",
    "    def transform_training_corpus(self):\n",
    "        small_words_corpus = [self._process_word(word) for word in self._training_data]\n",
    "        outs = np.zeros(len(small_words_corpus))\n",
    "        cap_words_corpus = [self._process_word((str(word).upper())) for word in self._training_data]\n",
    "        cap_outs = np.ones(len(cap_words_corpus))\n",
    "        y = np.expand_dims(np.append(outs, cap_outs),axis=1)\n",
    "        X = np.append(small_words_corpus, cap_words_corpus, axis=0)\n",
    "        return (X,y)\n",
    "    \n",
    "    def _process_word(self, word):\n",
    "        chars = ' '.join(list(str(word)))\n",
    "        return (vectorizer.transform([chars]).toarray().flatten())\n",
    "        \n",
    "        \n",
    "    def predict(self, para):\n",
    "        labels = [\"Not Abbrevation\", \"Abbrevation\"]\n",
    "        words = para.split(\" \")\n",
    "        vectors = [self._process_word(word) for word in words]\n",
    "        return [(words[idx], labels[int(preds)]) for idx,preds in enumerate(np.round(self.model.predict([vectors]).flatten()))]\n",
    "\n",
    "        #[(words[idx], labels[preds]) for idx,preds in enumerate(np.round(model.predict([vectors]).flatten()))]\n",
    "        \n",
    "    \n",
    "    def train(self ):\n",
    "        X,y = self.transform_training_corpus()\n",
    "        self.model.fit(x = X, y = y, batch_size = 64, verbose=0, epochs=40)\n",
    "        print('Training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "abbr = AbbrNetwork()\n",
    "abbr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pavan', 'Not Abbrevation'),\n",
       " ('is', 'Not Abbrevation'),\n",
       " ('new', 'Not Abbrevation'),\n",
       " ('to', 'Not Abbrevation'),\n",
       " ('PAN', 'Abbrevation')]"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abbr.predict(\"pavan is new to PAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
